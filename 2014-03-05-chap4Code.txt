This file is for adding the interesting code we find in our assigned chapters 1-4. Remember to comment on each piece of code so that we all know what it does. Also remember to make some cool combinations with the code you find (i.e. combining percentage code with the Freq distribution chart).

#Chapter 1


#Chapter 2


#Chapter 3


#Chapter 4

Here is what I have so far.
Note: I was on an online book that had all the chapters together, so some of this code may be in other chapters as well. If that is the case, feel free to delete these codes.


-------------- Code 1: Gender names and plot
#One more wordlist corpus is the Names corpus, containing 8,000 first names categorized by gender. The male and female names are stored in separate files. Let's find names which appear in both files, i.e. names that are ambiguous for gender:

# this code plots out the names that are male and female. It shows patterns based on the last letter of the name.

>>> names = nltk.corpus.names
>>> names.fileids()
('female.txt', 'male.txt')
>>> male_names = names.words('male.txt')
>>> female_names = names.words('female.txt')
>>> [w for w in male_names if w in female_names]
['Abbey', 'Abbie', 'Abby', 'Addie', 'Adrian', 'Adrien', 'Ajay', 'Alex', 'Alexis',
'Alfie', 'Ali', 'Alix', 'Allie', 'Allyn', 'Andie', 'Andrea', 'Andy', 'Angel',
'Angie', 'Ariel', 'Ashley', 'Aubrey', 'Augustine', 'Austin', 'Averil', ..]

 	
>>> cfd = nltk.ConditionalFreqDist((file, name[-1])
...           for file in names.fileids()
...           for name in names.words(file))
>>> cfd.plot()    		# see  a neat plot



-------------- Code 2: Program to search a wordlist to solve a word puzzle
#Issue, tried other letters bit keep getting '[]'

# We use the FreqDist comparison method to check that the frequency of each letter in the candidate word is less than or equal to the frequency of the corresponding letter in the puzzle

# Wold like to add a feature that gives the number of the resultant combinations

# This is great for scrabble

# Also if the user want to add any words, all they need to do is open the "word.txt" and add them into there.

>>> puzzle_letters = nltk.FreqDist('egivrvonl')
>>> obligatory = 'r'
>>> wordlist = nltk.corpus.words.words()
>>> [w for w in wordlist if len(w) >= 6
...                      and obligatory in w
...                      and nltk.FreqDist(w) <= puzzle_letters]
['glover', 'gorlin', 'govern', 'grovel', 'ignore', 'involver', 'lienor',
'linger', 'longer', 'lovering', 'noiler', 'overling', 'region', 'renvoi',
'revolving', 'ringle', 'roving', 'violer', 'virole']


-------------- Code 3: % of words that are not stopwords
# a function to compute what fraction of words in a text are not in the stopwords list:

>>> def content_fraction(text):
...     stopwords = nltk.corpus.stopwords.words('english')
...     content = [w for w in text if w.lower() not in stopwords]
...     return 1.0 * len(content) / len(text)
...
>>> content_fraction(nltk.corpus.reuters.words())  #here we can swap out corpus'
0.65997695393285261
>>> content_fraction(nltk.corpus.state_union.words())  #tried on the union corpus
0.5771401011575138


-------------- Code 4: Unusual words in a text
# This code gets a set of words from a text. 
# We can use it to find unusual or mis-spelt words in a text corpus

def unusual_words(text):
    text_vocab = set(w.lower() for w in text if w.isalpha())
    english_vocab = set(w.lower() for w in nltk.corpus.words.words())
    unusual = text_vocab.difference(english_vocab)
    return sorted(unusual)
    
unusual_words(nltk.corpus.gutenberg.words('austen-sense.txt'))
['abbeyland', 'abhorrence', 'abominably', 'abridgement', 'accordant', 'accustomary',...]
unusual_words(nltk.corpus.nps_chat.words())
['aaaaaaaaaaaaaaaaa', 'aaahhhh', 'abou', 'abourted', 'abs', 'ack', 'acros',...]
unusual_words(nltk.corpus.inaugural.words('1801-Jefferson.txt'))
['abuses', 'acknowledging', 'acquisitions', 'actions', 'administrations' ...]



-------------- Code 5: Languages most common wordlengths
# This uses a conditional frequency distribution to examine the differences in word lengths, for a selection of languages included in this corpus.

# Remember to download the Indian.txt form the Corpora

>>> nltk.corpus.cess_esp.words()
['El', 'grupo', 'estatal', 'Electricit\xe9_de_France', ...]
>>> nltk.corpus.floresta.words()
['Um', 'revivalismo', 'refrescante', 'O', '7_e_Meio', ...]
>>> nltk.corpus.udhr.words('Javanese-Latin1')[11:]
[u'Saben', u'umat', u'manungsa', u'lair', u'kanthi', ...]
>>> nltk.corpus.indian.words('hindi.pos')
['\xe0\xa4\xaa\xe0\xa5\x82\xe0\xa4\xb0\xe0\xa5\x8d\xe0\xa4\xa3', '\xe0\xa4\xaa\xe0\xa5\x8d\xe0\xa4\xb0\xe0\xa4\xa4\xe0\xa4\xbf\xe0\xa4\xac\xe0\xa4\x82\xe0\xa4\xa7', ...]
>>> from nltk.corpus import udhr
>>> languages = ['Chickasaw', 'English', 'German_Deutsch', 'Greenlandic_Inuktikut', 'Hungarian_Magyar', 'Ibibio_Efik']
>>> cfd = nltk.ConditionalFreqDist((lang, len(word))
...     for lang in languages
...     for word in udhr.words(lang + '-Latin1'))
>>> cfd.plot(cumulative=True)   		# See a neat graph that shows average word-length



-------------- Code 6: Plotting words used over time with the Inaugural Addresses
# This code takes the years in the Addresses and plots them against target words to see their change in usage. 

# Think about how to pair this with the word_percentage function

# Important! Remember to change all cases of .file()  to .fileids()
 	
>>> from nltk.corpus import inaugural
>>> inaugural.fileids()
('1789-Washington.txt', '1793-Washington.txt', '1797-Adams.txt', ...)
>>> [file[:4] for file in inaugural.fileids()]
['1789', '1793', '1797', '1801', '1805', '1809', '1813', '1817', '1821', ...]

 	
>>> cfd = nltk.ConditionalFreqDist((target, file[:4])
...           for file in inaugural.fileids()
...           for w in inaugural.words(file)
...           for target in ['america', 'citizen']
...           if w.lower().startswith(target))
>>> cfd.plot()  							# Get to see an neat plot!



-------------- Code 7: Drawing Trees
# A tree is a set of connected nodes, each of which is labeled with a category. It common to use a 'family' metaphor to talk about the relationships of nodes in a tree: for example, s is the parent of vp; conversely vp is a daughter (or child) of s. Also, since np and vp are both daughters of s, they are also sisters. Here is an example of a tree:


>>> import nltk
>>> import nltk, re, pprint
>>> tree1 = nltk.Tree('NP', ['John'])
>>> print tree1
(NP John)
>>> tree2 = nltk.Tree('NP', ['the', 'man'])
>>> print tree2
(NP the man)
>>> tree3 = nltk.Tree('VP', ['saw', tree2])
>>> tree4 = nltk.Tree('S', [tree1, tree3])
>>> print tree4
(S (NP John) (VP saw (NP the man)))
>>> print tree4[1]
(VP saw (NP the man))
>>> tree4[1].node
'VP'
>>> tree4.leaves()
['John', 'saw', 'the', 'man']
>>> tree4[1,1,1]
'man'
>>> tree3.draw()   # The tree display window allows you to zoom in and out; to collapse and expand subtrees; and to print the graphical representation to a postscript file (for inclusion in a document).



-------------- Code 8: Ave of sentence in Brown corpus, by category
# needs debugging

>>> lengths = map(len, nltk.corpus.brown.sents(categories='news'))
>>> sum(lengths) / float(len(lengths))
21.7508111616



-------------- Code 9: See a graph of words in genres
# Needs debugging

>>> import nltk
>>> colors = 'rgbcmyk' # red, green, blue, cyan, magenta, yellow, black
>>> def bar_chart(categories, words, counts):
...     "Plot a bar chart showing counts for each word by category"
...     import pylab
...     ind = pylab.arange(len(words))
...     width = 1 / (len(categories) + 1)
...     bar_groups = []
...     for c in range(len(categories)):
...             bars = pylab.bar(ind+c*width, counts[categories[c]], width,
...                     color=colors[c % len(colors)])
...             bar_groups.append(bars)
...     pylab.xticks(ind+width, words)
...     pylab.legend([b[0] for b in bar_groups], categories, loc='upper left')
...     pylab.ylabel('Frequency')
...     pylab.title('Frequency of Six Modal Verbs by Genre')
...     pylab.show()
... 
>>> genres = ['news', 'religion', 'hobbies', 'government', 'adventure']
>>> modals = ['can', 'could', 'may', 'might', 'must', 'will']
>>> cfdist = nltk.ConditionalFreqDist(
...     (genre, word)
...     for genre in genres
...     for word in nltk.corpus.brown.words(categories=genre)
...     if word in modals)
>>> counts={}
>>> for genre in genres:
...     counts[genre] = [cfdist[genre][word] for word in modals]
...     
... 
>>> bar_chart(genres, modals, counts)



-------------- Code 10: Find the longest word in a text or corpus
# For the first word in a set of works:	
# Searching in a corpuses
text = nltk.corpus.gutenberg.words('milton-paradise.txt')					
longest = ''				
for word in text:				
	if len(word) > len(longest):
		longest = word			
								
logest							
('unextinguishable')	

#(have to do these together)

Find all the longest words:

>>> maxlen = max(len(word) for word in text)
>>> [word for word in text if len(word) == maxlen]
['unextinguishable', 'transubstantiate', 'inextinguishable', 'incomprehensible']

# Searching in a text:

For the first word in a set of works:	
>>> text = text4
>>> longest = ''
>>> for word in text:
...     if len(word) > len(longest):
...             longest = word
... 
>>> longest
'antiphilosophists'

Find all the longest words:
>>> maxlen = max(len(word) for word in text)
>>> [word for word in text if len(word) == maxlen]
['antiphilosophists', 'misrepresentation', 'contradistinction', 'misrepresentation', 'instrumentalities', 'instrumentalities', 'instrumentalities', 'instrumentalities', 'instrumentalities', 'instrumentalities']



-------------- Code 11: Text from the web
# resource: http://www.slideshare.net/pbpimpale/natural-language-toolkit-nltk-basics / slide#39

# getting the type and length of characters in the online text
import urllib
import urllib2
>>> url = 'http://www.gutengerg.org/files/2554/2554.txt'
>>> raw=urlopen(url).read()
>>> type(raw)
<type 'str'>
>>> sort(len(raw))
22282  #is in characters

# getting the html from the online text

>>> req = urllib2.Request(url, headers ={'User-Agent':'Chrome'})
>>> raw = urllib2.urlopen(req).read()
>>> response = urllib2.urlopen(request)
>>> html = response.read()
>>> print html


-------------- Code 12: User input and tokenization
# This tokenizes the user's input text.
>>> s = raw_input('Enter some text: ')
Enter some text: Agatha Christy is a weird's sisters, but she does write some pretty trippy books.
>>> print 'You typed', len(nltk.word_tokenize(s)), 'words.'
You typed 17 words.


-------------- Code 13: Stemming text
# This stems all the words
>>> text=['apples', 'called', 'indian', 'applying']
>>> porter=nltk.PorterStemmer()
>>> lancaster=nltk.LancasterStemmer()
>>> [porter.stem(p) for p in text]
['appl', 'call', 'indian', 'appli']
>>> [lancaster.stem(p) for p in text]
['appl', 'cal', 'ind', 'apply']




----------------  ISSUES ---------------

Need to download the networkx 1.8.1
https://pypi.python.org/pypi/networkx/

import networkx as nx